% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{url} 
\usepackage[OT1]{fontenc} 
\usepackage{fontspec}
\usepackage{float}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[bottom]{footmisc}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\groupID{Group-Q}
\def\subjectNum{COMP6721}
\def\subName{Applied AI } 



\begin{document}
\definecolor{barblue}{RGB}{206,34,34}
\definecolor{groupblue}{RGB}{120,34,34}
\definecolor{linkred}{RGB}{165,0,33}
% \renewcommand\sfdefault{phv}
% \renewcommand\mddefault{mc}
% \renewcommand\bfdefault{bc}
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{Group-Q} % *** Enter the CVPR Paper ID here
\def\confName{COMP6721}
\def\confYear{2022}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Lung Disease Clasification - \subName Progress Report}
\author{Rohan Chopra\\
\small 40233019\\
\and
Harman Singh Jolly\\
\small 40204947\\
\and
Harman Preet Kaur\\
\small 40198317\\
\and
Abhishek Handa\\
\small 40231719\\
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
  The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, 
  below the author and affiliation information.
  Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative 
  to the column, initially capitalized.
  The abstract is to be in 10-point, single-spaced type.
  Leave two blank lines after the Abstract, then begin the main text.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts to get a feel for style and length.
  Look at previous CVPR abstracts.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction-1.5}
\label{sec:intro}

Early diagnosis of respiratory diseases like pneumonia and COVID-19 leads to decreased mortality 
rate \cite{daniel2016time} and is a powerful way to manage a pandemic\cite{xu2020facile}. 
These diseases can be diagnosed using a variety of tests like pulse oximetry, chest x-ray, 
CT scan\cite{mattsmith2022},  PCR\cite{akhtar1996pcr} however chest X-rays are by far the most 
accessible\cite{frija2021improve} to low and middle income countries. 
Furthermore, the scan is available in minutes making it one of the fastest ways of diagnosis
\cite{healthwise2021}. However, the bottleneck with this method 
is the need for an expert radiologists to evaluate the scan\cite{mehrotra2009radiologists}. 
Many researchers have tried to solve this problem by creating a deep learning based
lung disease classification system \cite{wang2021deep} but 
haven't been able to come up with models that can replace radiologists. Small 
\cite{guefrechi2021deep} and highly imbalanced data \cite{wang2021deep}, along with varying 
specifications of X-ray scanners leading to low inter-hospital accuracy \cite{melissarohman2018}
are the biggest problems that 
researchers have faced. Another issue with using deep neural networks in medical settings is 
its black-box nature\cite{paulblazek2022}, doctors and patients will not trust a model that 
cannot explain its results\cite{aleksandra2019}. 

This project is an attempt to compare three CNN backbone architectures namely, ResNet-34, 
MobileNet V3 Large and EfficientNet B1 along with three lung disease datasets 
to identify the type of architecture that works best for lung disease classification. Two 
of the datasets used presented a multiclass classification problem with 3 classes while 
the third dataset presented a multiclass, multilabel classification problem. A total 
of 12 models were trained in this study, four for each of the three datasets. The first 
three models for each dataset was trained from scratch and the fourth model was trained 
using transfer learning. Transfer learning was performed by deep-tuning ImageNet
weights and the performance was evaluated to check improvement over the models trained from scratch. 
The small dataset problem and the issue of different radiographic contrast \cite{andrew2022rad} 
is mitigated using data augmentation. Imbalanced data problem is handled by undersampling the 
majority class. The hyperparameters were fixed across models and the F1 scores and cross entropy loss 
have been used to compare models and select the best overall model. All the models were optimized using 
the Adam optimizer \cite{kingma2014adam} with default parameters and the cosine annealing \cite{loshchilov2016sgdr} 
learning rate scheduler was used to decrease the learning rate as training progressed.
Further, an ablation study was performed to find the best learning rate for the selected model.
Finally, GradCAM \cite{jacobgilpytorchcam} and T-SNE were used to visualize the trained models and understand 
model predictions better. An F1 score of 0.8 and 0.98 was achieved for the two multiclass
datasets, whereas the maximum F1 for the multilabel dataset with 7 classes was 0.46.

\textbf{Related Works:} 
Li \etal [] were among the first
to use CNNs in a medical setting. They used a single convolutional layer to classify 
interstitial lung diseases using CT scans, achieving better performance than existing 
approaches. Since then there has been a dramtic increase in application of CNNs 
in healthcare, deep neural networks have been used to perform various tasks like 
segmenting regions of interest in MRI [,], classifying X-Ray  
[], MRI [], and CT [] scans.
Further, GANs have been used to generate high quality scans [] 
when there is a lack of available data due to either privacy reasons or availability of subjects. 
GANs have also been used to generate high quality CT scans from MRI scans 
[]. Apart from radiographic scans,
deep CNNs have also been used to detect malarial parasite in blood smear images 
[] with an accuracy of 99.96\%. Another interesting 
application is the use of 1-D convolutions to detect heart anomalies using ECG data
[]. Researchers have also used architectures like the Inception V3 to perform 
dermatologist level skin-cancer detection using skin lesion images [] using transfer 
learning.  

In the recent years, many researchers have tried to predict lung diseases using deep CNNs, 
Wang \etal \cite{wang2017chestx} used state of the art backbone architectures to train 
a lung disease classifier for multilabel data by training only the prediction and transition 
layers from scratch and leaving pre-trained ImageNet weights freezed while training. 
They achieved a high AUC of over 0.6 for most of the classes in the dataset with this technique.
Rajpurkar \etal [] created a 121 layer deep CNN - CheXNet to detect
pneumonia using chest X-rays with radiologist level accuracy. 
Labhane \etal [] used transfer learning with state of the art backbone architectures like VGG16, 
VGG19 and InceptionV3 to predict pneumonia in pediatric patients and achieved an F1 score of 0.97.
Islam \etal combined CNN and LSTM 
to create a COVID-19 detector []. The CNN extracted complex features from scans and the LSTM was 
used as a classifier. This method resulted in an improvement over a vanilla CNN network and 
an F1 score of 98.9\% was achieved. Abbas \etal [] created the 
DeTraC network to detect COVID in chest X-rays that improved performance of existing backbone 
models significantly with the highest accuracy of 98.23\% using the VGG19 architecture.  
Guefrechi \etal [] on the other hand used data augmentation techniques like random rotation, 
flipping and noise with transfer learning on backbone architectures like ResNet50, 
InceptionV3 and VGG16 to achieve a high accuracy of 98.3\%.

In the following sections methodology of the approach and the results will be discussed.



%-------------------------------------------------------------------------
\section{Methodology-2}
\label{sec:prop_method}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{pre_proc_img.eps}  
   \caption{Effect of pre-processing on Chest X-ray images.}
   %\vspace{-1em}
   \label{fig:pre_proc_img}
\end{figure}



\begin{figure}
  \subfloat[COVID Dataset]{\includegraphics[width = 1\linewidth]{samp_img.eps}}\\
  \subfloat[Pneumonia Dataset]{\includegraphics[width = 1\linewidth]{abhishek_dataset_comparison.eps}}\\
  \subfloat[Chest X-Ray 8 Dataset]{\includegraphics[width = 1\linewidth]{rohan_dataset_comparison.eps}}
  \caption{Sample Chest X-rays from the datasets used.}
  \label{fig:sample_imgs}
  \end{figure}

\begin{table}
  \centering
  \begin{tabular}{p{2.9cm}|p{2.3cm}|wc{0.8cm}|p{0.8cm}}
    \toprule
    Dataset & No. of Images & Classes & Size\\
    \midrule
    COVID\cite{RAHMAN2021104319,9144185,kagglecovid} & 10000k:3.6k:1.3k & 3 & 299\textsuperscript{2}\\
    \midrule
    Pneumonia\cite{kermany2018labeled,kagglepneu} & 3k:1.5k:1.5k & 3 & 224\textsuperscript{2}\\
    \midrule
    Chest X-Ray8\cite{wang2017chestx,kaggle8} & 7.2k:7k:7k:4.1k :3.9k:3.5k:2.9k & 7 & 1024\textsuperscript{2}\\
    \bottomrule
  \end{tabular}
  \caption{Shortlisted Datasets.}
  %\vspace{-1.5em}
  \label{tab:selDataset}
\end{table}

\textbf{Datasets:} (\cref{tab:selDataset}) with varying disease types were chosen to ensure 
model robustness. Other criteria included the \textit{number of images per class} and 
\textit{image quality} as noisy scans can lead to mis-diagnosis\cite{sivakumar2012computed}. 
The \textbf{COVID} dataset was created using 43 \cite{covidpneumonia2020data} different 
publications. \cite{RAHMAN2021104319,9144185,kagglecovid} X-rays with widespread, hazy, 
and irregular ground glass opacities are of the COVID-19 class \cite{jacobi2020portable}. 
Whereas, the ones with haziness only in the lower regions \cite{zhan2021clinical} are 
viral pneumonia cases as shown in \cref{fig:sample_imgs}.
The \textbf{Pneumonia}, dataset contains scans from pediatric patients of one to five year olds 
collected as part of patients' routine clinical care. 
\cite{kermany2018labeled,kagglepneu} Scans with one white condensed area affecting only one 
side of the lungs are tagged as bacterial pneumonia\cite{areviral}. Whereas, X-rays which 
show bilateral patchy areas of consolidation are classified as viral pneumonia
\cite{guo2012radiological}. 
The \textbf{Chest X-ray 8} dataset was released by NIH \cite{chestxray2017data} with over 100k 
chest X-ray images and their radiological reports which Wang \etal \cite{wang2017chestx} 
used to create disease labels through NLP. \cite{kaggle8} It contains 15 classes but only 
7 were chosen for this study. This dataset is significantly different from the 
other two as it is a multilabel dataset. Classes were iteratively removed, ensuring that
classes are not highly imbalanced to finally reach the 7 classes. With over 29,000 images of size 
1024 x 1024, this dataset was the biggest and thus had to be resized down to 384 x 384 to 
reduce training time. Furthermore, normal class images were undersampled to choose only 7000 scans.
The data consisted of multiple scans from the same subject which would lead to data leakage between 
the train, val and test sets. This was prevented with the use of GroupShuffleSplit from the scikit
library. 

Before training, all the images were pre-processed using histogram equalization and Gaussian blur 
with a 5x5 filter as Gie≈Çczyk \etal \cite{gielczyk2022pre} showed that this improved the 
F1 score by about 4\% for chest X-ray classification. Visually, the contrast of the scan improved 
and allowed irregularities to stand out as shown in \cref{fig:pre_proc_img}. Next, the 
scans were divided into train, validation and test with the 70:15:15 split. 
During training, the scans were augmented using RandomHorizontalFlip, RandomAdjustSharpness, and 
RandomAutocontrast in Pytorch \cite{transforms} to increase the number of images the 
model gets to learn from and ensure that the model is robust to scans from different machines.

\textbf{Backbone Architectures:} (\cref{tab:selArch}) of various configuration and blocks were chosen. Other selection criteria were the \textit{number of trainable parameters}, important as total training time and hardware resources are limited for this project and the \textit{top 5 classification accuracy} on the ImageNet 1K benchmark dataset.
\begin{table}
  \centering
  \begin{tabular}{p{1.7cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}}
  \toprule
  Arch. & Params (Mil.) & Layers & FLOPS (Bil.) & Imagenet Acc.\\
  \midrule
  MobileNet & 5.5 & 18 & 8.7 & 92.6\\
  \midrule
  EfficientNet & 7.8 & 25 & 25.8 & 94.9\\
  \midrule
  Resnet & 21.8 & 34 & 153.9 & 91.4\\
  \bottomrule
  \end{tabular}
  \caption{Shortlisted Backbone Architectures.}
  %\vspace{-1.5em}
  \label{tab:selArch}
\end{table}
\textbf{ResNet 34}: residual learning network with 34 layers that are made possible by skip 
connections. The 34 layer variant was chosen to decrease training time for this study. 
\cite{he2016deep}
\textbf{MobileNet V3 Large}: uses depthwise separable convolution from MobileNet V2 
\cite{sandler2018mobilenetv2} along with squeeze-excitation blocks in residual layers 
from MnasNet \cite{tan2019mnasnet}. Howard \etal \cite{howard2019searching} also used 
network architecture search to find the most effective model. The large configuration 
was chosen to not compromise on the prediction accuracy.
\textbf{EfficientNet B1}: uses compound scaling to scale the model by depth, width and 
resolution. The B1 version was chosen to have faster training without compromising on the 
accuracy. \cite{tan2019efficientnet}

\textbf{Optimization Algorithm:}
The Adam optimizer \cite{kingma2014adam} was chosen as the algorithm of choice as it converges 
faster on image classification tasks and does not requires little tuning. It integrates benefits of  
RMSProp and Adagrad to produce robust results on a wide range of problems.

\section{Results-2.5} 
\label{sec:method}
\textbf{Experiment Setup:}

Two datasets in this study had a very small number of samples which caused the models to 
overfit early. To mitigate this, random contrast and sharpness adjustment 
\cite{nanni2021comparison} data augmentation techniques were used. Some scans in the 
datasets were anterior-posterior while some others were posterior-anterior and using the 
horizontal flip data augmentation would make the model invariant to these differences
\cite{botev2022regularising}.
Inception was the first model trained and each epoch took over 1 hour. To reduce the 
training time, the X-ray images were resized, pre-processed and split into train, test and 
validation sets separately. Furthermore, EfficientNet, MobileNet and ResNet 34 were chosen 
as they have a considerably low number of learnable parameters. Now each epoch is taking 
less than 4 minutes. 

\begin{table*}[tbh]
  \centering
  \boldmath
  \scalebox{0.9}{%
  \begin{tabular}{l|ccc|ccc|ccc|ccc} \hline
    Model & \multicolumn{3}{c|}{\textbf{ResNet}} & \multicolumn{3}{c|}{\textbf{MobileNet}} & \multicolumn{3}{c|}{\textbf{EfficientNet}} & \multicolumn{3}{c}{\textbf{EN - Transfer Learning}} \\
  \cline{1-13}
  Dataset & \emph{F1} & \emph{Time} & \emph{Epoch} & \emph{F1} & \emph{Time} & \emph{Epoch} & \emph{F1} & \emph{Time} & \emph{Epoch} & \emph{F1} & \emph{Time} & \emph{Epoch}  \\
   \hline
  \emph{Pneumonia}           & 0.784 & 82 &  \textbf{22} &   \textbf{0.804} & \textbf{75} &  42 &   0.768 & 110 &  44 &   0.782 & 114 &  70  \\
  \hline
  \emph{COVID}        & 0.963 & 68 & 71 & 0.959 & \textbf{45} & 82 & 0.970 & 80 & 89 & \textbf{0.978} & 56 & \textbf{46}   \\ % \hdashline
  \hline
  \emph{X-Ray 8} &   0.411 & 11,502 & \textbf{19} & 0.406 & \textbf{7,275} & 42 & 0.445 & 13,820 & 31 & \textbf{0.457} & 13,813 & 29   \\ \hline
  \end{tabular}
  }
  \caption{F1 (higher is better), time per epoch in seconds (lower is better), and number of epochs to reach the best validation loss (lower is better) for the 12 models that were trained.}
  \label{table:conducts}
  \end{table*}
  \begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{f1_loss.eps}  
     \caption{Train \& Val F1 \& Loss plots for the 9 models.}
     %\vspace{-1em}
     \label{fig:acc_loss_sep}
  \end{figure}
  
  \begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{tl_f1_loss.eps}  
     \caption{Train \& Val, F1 \& Loss plots for EfficientNet trained from scratch and with ImageNet weights.}
     %\vspace{-1em}
     \label{fig:tl_acc_loss}
  \end{figure}
Nine models were trained from scratch and the training, validation F1 score and loss can be 
seen in \cref{fig:acc_loss_sep}. From the plots it is clear that going from a smaller 
architecture to a bigger architecture, makes the model start to overfit earlier. Another 
interesting observation is that cosine annealing impacted the loss of MobileNet the most 
every 10 epochs due to warm restarts. From the graphs it can be seen that all three 
datasets had similar performance across models when trained for a high number of epochs. 
The X-ray 8 dataset performed the worst among the three datasets which could be due to the 
high number of classes as compared to the other datasets. Surprisingly, the pneumonia 
dataset performed worse than the COVID + pneumonia dataset which indicates that COVID 
cases are easier to distinguish from pneumonia cases. 

\textbf{Main Results:}

\begin{figure}
  \subfloat[Resnet]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_resnet.eps}}
  \subfloat[MobileNet]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_mobilenet.eps}}\\
  \subfloat[EfficientNet]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_efficientnet.eps}}
  \subfloat[EN - Transfer Learning]{\includegraphics[width = 0.5\linewidth]{tsne_pneumonia_efficientnet_tl.eps}}
  \caption{T-SNE and Confusion matrics of the Pneumonia dataset.}
  \label{fig:tsne_pneumonia}
\end{figure}

\begin{figure}
  \subfloat[Resnet]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_resnet.eps}}
  \subfloat[MobileNet]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_mobilenetnet.eps}}\\
  \subfloat[EfficientNet]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_efficientnet.eps}}
  \subfloat[EN - Transfer Learning]{\includegraphics[width = 0.5\linewidth]{tsne_covid_pneumonia_efficientnet_tl.eps}}
  \caption{T-SNE and Confusion matrics of the COVID dataset.}
  \label{fig:tsne_covid}
\end{figure}

\begin{figure}
  \subfloat[Resnet]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_resnet.eps}}
  \subfloat[MobileNet]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_mobilenet.eps}}\\
  \subfloat[EfficientNet]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_efficientnet.eps}}
  \subfloat[EN - Transfer Learning]{\includegraphics[width = 0.5\linewidth]{tsne_xray8_efficientnet_tl.eps}}
  \caption{T-SNE and Confusion matrics of the Chest X-ray 8 dataset.}
  \label{fig:tsne_xray8}
\end{figure}

\textbf{Ablative Study:}



\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{ablation_plot.eps}  
   \caption{Train \& Val, F1 \& Loss plots for ablative study models.}
   %\vspace{-1em}
   \label{fig:ablation_plot}
\end{figure}
\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{ablation.eps}  
   \caption{Ablative Study F1 scores (Higher is better).}
   %\vspace{-1em}
   \label{fig:ablation}
\end{figure}



%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}