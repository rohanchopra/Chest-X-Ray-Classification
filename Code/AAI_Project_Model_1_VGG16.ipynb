{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AQ2e4ZisZqHz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.utils.data as td\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot\n",
        "import time\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms.functional import normalize\n",
        "from torchvision.transforms.transforms import ToTensor\n",
        "from torchvision.transforms.transforms import Resize\n",
        "from torchvision.transforms.transforms import RandomHorizontalFlip\n",
        "from torchvision.transforms.transforms import Normalize\n",
        "\n",
        "\n",
        "PATH_TO_DATASET=\"/content/drive/MyDrive/chest_xray/train\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path, test_split, val_split, batch_size, input_size):\n",
        "    \n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "    ######## Write your code here ########\n",
        "    transform_dict={\"src\":transforms.Compose([\n",
        "      transforms.Resize((227,227)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize])}\n",
        "      \n",
        "    data=torchvision.datasets.ImageFolder(root=PATH_TO_DATASET,transform=transform_dict[\"src\"])\n",
        "#dataset = TensorDataset(x_tensor, y_tensor)\n",
        "#val_size = int(len(dataset)*0.2)\n",
        "#train_size = len(dataset)- int(len(dataset)*0.2)\n",
        "#train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    train_split=1-test_split-val_split\n",
        "    train_size=int(len(data)*train_split)\n",
        "    val_size=int(len(data)*val_split)\n",
        "    test_size=int(len(data)*test_split)\n",
        "    print(\"The train size, val size and test size is resp \",train_size,\" \",val_size,\" \",test_size,\" \",len(data))\n",
        "    train,val,test=torch.utils.data.random_split(data,[train_size,val_size,test_size])\n",
        "    data_loader_train = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0, drop_last=False)\n",
        "    data_loader_test = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0, drop_last=False)\n",
        "    data_loader_val = torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0, drop_last=False)\n",
        "    return data_loader_train, data_loader_test, data_loader_val\n"
      ],
      "metadata": {
        "id": "o-kdhS-VaMwq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader_train, data_loader_test, data_loader_val=load_data(PATH_TO_DATASET,0.25,0.25,32,40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-euDBI-7bf-c",
        "outputId": "e40ff5d2-b1e5-4a8a-8118-2d2b4ae48d54"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The train size, val size and test size is resp  2928   1464   1464   5856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(7*7*512, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ySiTvA7YdZdS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = 3\n",
        "num_epochs = 20\n",
        "batch_size = 16\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = VGG16(num_classes).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(data_loader_train)"
      ],
      "metadata": {
        "id": "20pWEpMldg8C"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_step = len(data_loader_train)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(data_loader_train):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in data_loader_train:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "    \n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qjZ-DURfDzs",
        "outputId": "6d639c93-7134-4912-8345-c975c387d383"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [92/92], Loss: 0.4475\n",
            "Accuracy of the network on the 5000 validation images: 74.21448087431693 %\n",
            "Epoch [2/20], Step [92/92], Loss: 1.0172\n",
            "Accuracy of the network on the 5000 validation images: 70.86748633879782 %\n",
            "Epoch [3/20], Step [92/92], Loss: 0.5944\n",
            "Accuracy of the network on the 5000 validation images: 75.03415300546447 %\n",
            "Epoch [4/20], Step [92/92], Loss: 0.5565\n",
            "Accuracy of the network on the 5000 validation images: 82.00136612021858 %\n",
            "Epoch [5/20], Step [92/92], Loss: 0.4853\n",
            "Accuracy of the network on the 5000 validation images: 81.4207650273224 %\n",
            "Epoch [6/20], Step [92/92], Loss: 0.3276\n",
            "Accuracy of the network on the 5000 validation images: 80.94262295081967 %\n",
            "Epoch [7/20], Step [92/92], Loss: 0.6611\n",
            "Accuracy of the network on the 5000 validation images: 80.9084699453552 %\n",
            "Epoch [8/20], Step [92/92], Loss: 0.5165\n",
            "Accuracy of the network on the 5000 validation images: 82.71857923497268 %\n",
            "Epoch [9/20], Step [92/92], Loss: 0.9424\n",
            "Accuracy of the network on the 5000 validation images: 84.2896174863388 %\n",
            "Epoch [10/20], Step [92/92], Loss: 0.3306\n",
            "Accuracy of the network on the 5000 validation images: 83.7431693989071 %\n",
            "Epoch [11/20], Step [92/92], Loss: 0.3584\n",
            "Accuracy of the network on the 5000 validation images: 85.82650273224044 %\n",
            "Epoch [12/20], Step [92/92], Loss: 0.3819\n",
            "Accuracy of the network on the 5000 validation images: 86.23633879781421 %\n",
            "Epoch [13/20], Step [92/92], Loss: 0.2457\n",
            "Accuracy of the network on the 5000 validation images: 85.99726775956285 %\n",
            "Epoch [14/20], Step [92/92], Loss: 0.3310\n",
            "Accuracy of the network on the 5000 validation images: 86.68032786885246 %\n",
            "Epoch [15/20], Step [92/92], Loss: 0.2645\n",
            "Accuracy of the network on the 5000 validation images: 88.01229508196721 %\n",
            "Epoch [16/20], Step [92/92], Loss: 0.4058\n",
            "Accuracy of the network on the 5000 validation images: 89.48087431693989 %\n",
            "Epoch [17/20], Step [92/92], Loss: 0.3109\n",
            "Accuracy of the network on the 5000 validation images: 86.06557377049181 %\n",
            "Epoch [18/20], Step [92/92], Loss: 0.4135\n",
            "Accuracy of the network on the 5000 validation images: 83.67486338797814 %\n",
            "Epoch [19/20], Step [92/92], Loss: 0.0955\n",
            "Accuracy of the network on the 5000 validation images: 89.24180327868852 %\n",
            "Epoch [20/20], Step [92/92], Loss: 0.2431\n",
            "Accuracy of the network on the 5000 validation images: 91.97404371584699 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in data_loader_test:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCAkQbqRuADZ",
        "outputId": "8a1aa104-5203-47bc-a8f6-67992764191a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 80.39617486338798 %\n"
          ]
        }
      ]
    }
  ]
}