# -*- coding: utf-8 -*-
"""chest-x-ray8-pre-processing (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z2kaFJeSkpmFxaEhaWW1QtvPp7BvTePI
"""

from google.colab import drive

drive.mount('/content/drive')

import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch.utils.data as td
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from matplotlib import image
from matplotlib import pyplot
import time

import torchvision
from torchvision.transforms.functional import normalize
from torchvision.transforms.transforms import ToTensor
from torchvision.transforms.transforms import Resize
from torchvision.transforms.transforms import RandomHorizontalFlip
from torchvision.transforms.transforms import Normalize

PATH_TO_DATASET = "/content/drive/MyDrive/chest_xray/train"


def load_data(path, test_split, val_split, batch_size, input_size):
    ######## Write your code here ########
    transform_dict = {"src": transforms.Compose([
        Resize((input_size, input_size)),
        RandomHorizontalFlip(p=0.5),
        ToTensor(),
        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}

    data = torchvision.datasets.ImageFolder(root=PATH_TO_DATASET, transform=transform_dict["src"])
    # dataset = TensorDataset(x_tensor, y_tensor)
    # val_size = int(len(dataset)*0.2)
    # train_size = len(dataset)- int(len(dataset)*0.2)
    # train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    train_split = 1 - test_split - val_split
    train_size = int(len(data) * train_split)
    val_size = int(len(data) * val_split)
    test_size = int(len(data) * test_split)
    print("The train size, val size and test size is resp ", train_size, " ", val_size, " ", test_size, " ", len(data))
    train, val, test = torch.utils.data.random_split(data, [train_size, val_size, test_size])
    data_loader_train = torch.utils.data.DataLoader(train, batch_size=batch_size,
                                                    shuffle=True, num_workers=0, drop_last=False)
    data_loader_test = torch.utils.data.DataLoader(test, batch_size=batch_size,
                                                   shuffle=True, num_workers=0, drop_last=False)
    data_loader_val = torch.utils.data.DataLoader(val, batch_size=batch_size,
                                                  shuffle=True, num_workers=0, drop_last=False)
    return data_loader_train, data_loader_test, data_loader_val


path = PATH_TO_DATASET
######## Write your code here ########
normal = plt.imread(path + '/NORMAL' + '/NORMAL-28501-0001.jpeg')
pneumonia = plt.imread(path + '/PNEUMONIA' + '/BACTERIA-7422-0001.jpeg')
plt.subplot(211)
plt.imshow(normal)
plt.subplot(212)
plt.imshow(pneumonia)

data_loader_train, data_loader_test, data_loader_val = load_data(path, 0.25, 0.25, 32, 40)


def plot_data_loader(data_loader, gridDims):
    fig, axes = plt.subplots(nrows=gridDims[0], ncols=gridDims[1], figsize=(5, 5))
    dataiter = iter(data_loader)
    for i in range(gridDims[0]):
        for j in range(gridDims[1]):
            images, _ = dataiter.next()
            axes[i, j].imshow(np.transpose(images[0].numpy(), (1, 2, 0)))


plot_data_loader(data_loader_train, [2, 2])


class CNN(nn.Module):
    def __init__(self, inputDims):
        super(CNN, self).__init__()
        self.conv_layer = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(inplace=True),
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(inplace=True),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.fc_layer = nn.Sequential(
            nn.Dropout(p=0.1),
            # nn.Linear(int(inputDims[0]/4) * int(inputDims[1]/4) * 64, 1000),
            nn.Linear(6400, 1000),
            nn.ReLU(inplace=True),
            nn.Linear(1000, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.1),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        # conv layers
        x = self.conv_layer(x)
        # flatten
        x = x.view(x.size(0), -1)
        # fc layer
        x = self.fc_layer(x)
        return x


model = CNN((32, 32))
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Device: {}".format(device))
model.to(device)

num_epochs = 10
total_steps = len(data_loader_train)
t1 = time.time()
for epoch in range(num_epochs):
    for i, data in enumerate(data_loader_train):
        images, labels = data[0].to(device), data[1].to(device)
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        # Backprop and optimisation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # Train accuracy
        total = labels.size(0)
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        if (i + 1) % 10 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'
                  .format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),
                          (correct / total) * 100))

print("######## Training Finished in {} seconds ###########".format(time.time() - t1))

model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for data in data_loader_val:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    print('Test Accuracy of the model on the {} test images: {} %'
          .format(total, (correct / total) * 100))

model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for data in data_loader_test:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    print('Test Accuracy of the model on the {} test images: {} %'
          .format(total, (correct / total) * 100))